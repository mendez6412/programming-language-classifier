{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Language Am I Anyway\n",
    "Where the rules are naive and the scores don't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as tuff\n",
    "from sklearn.pipeline import Pipeline\n",
    "from extensions import extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Storing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_prog_files(loc):\n",
    "    files = glob.glob(loc, recursive=True)\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file, encoding='latin_1') as f:\n",
    "            texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing samples (for training) and their appropriate labels into lists at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "\n",
    "for ext, name in extensions.items():\n",
    "    x = read_prog_files('bmgame/bmgame/bench/**/*.{}'.format(ext))\n",
    "    samples += x\n",
    "    y = len(x) * [name]\n",
    "    labels += y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline with a CountVectorizer and Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip = Pipeline([('cv', CountVectorizer(analyzer='word', token_pattern=r'[a-zA-Z]{2,}|\\s{2,}|[^\\w\\d\\s]')), ('bay', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting my pipeline with samples (code snippet) and labels (code type)\n",
    "The score of samples to labels is as expected (close to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97933227344992047"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.fit(samples, labels)\n",
    "pip.score(samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a train_test_split to ensure a good set of data\n",
    "My test lists score fairly well (.87) and am feeling somewhat confident going forward with this fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84523809523809523"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_x, train_y, test_y = train_test_split(samples, labels, train_size=.6, random_state=42)\n",
    "pip.fit(train_X, train_y)\n",
    "pip.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Unknown Testing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown = []\n",
    "ulabels = []\n",
    "for item in range(1, 33):\n",
    "    x = read_prog_files('test/{}'.format(item))\n",
    "    unknown += x\n",
    "with open('test.csv') as testy:\n",
    "    reader = csv.reader(testy)\n",
    "    for row in reader:\n",
    "        ulabels.append(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring my unseen code snippets to the correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.score(unknown, ulabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clojure', 'clojure', 'clojure', 'clojure', 'ruby', 'python',\n",
       "       'python', 'python', 'scala', 'javascript', 'javascript', 'scala',\n",
       "       'ruby', 'javascript', 'ruby', 'haskell', 'haskell', 'tcl', 'scheme',\n",
       "       'scheme', 'scheme', 'c', 'javascript', 'scala', 'scala', 'tcl',\n",
       "       'tcl', 'c', 'php', 'php', 'ocaml', 'ocaml'], \n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.predict(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Why am I only getting around .75?\n",
    "Here I am counting my set of labels that I fitted my pipeline with... perhaps more sampling would improve my model... that usually seems to be the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_d = {}\n",
    "for item in labels:\n",
    "    count_d.setdefault(item, 0)\n",
    "    count_d[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 59,\n",
       " 'c#': 41,\n",
       " 'clojure': 38,\n",
       " 'common lisp': 34,\n",
       " 'haskell': 43,\n",
       " 'java': 51,\n",
       " 'javascript': 43,\n",
       " 'ocaml': 35,\n",
       " 'perl': 34,\n",
       " 'php': 55,\n",
       " 'python': 36,\n",
       " 'ruby': 59,\n",
       " 'scala': 43,\n",
       " 'scheme': 29,\n",
       " 'tcl': 29}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_language(code):\n",
    "    return pip.predict([code])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_language('numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
